{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Programming Lab\n",
    "This lab is an introduction of GPU programming with CUDA using python. It consists of 5 exercises plus homework. Write all your observations during the lab exercises and the homework in the report section at the bottom. Be brief and efficient. Send the report to Abdallah Alabdallah <abdallah.alabdallah@hh.se> by Friday, 1 May 2020 at 23:59.\n",
    "\n",
    "# CUDA\n",
    "CUDA is a parallel programming platform and an API that facilitates access to the CUDA-Enabled GPU functionality for general-purpose computing. It allows speeding up the software by utilizing the GPU power for the parallelizable part of the computation. Many Deep Learning platforms like TensorFlow, Keras, PyTorch and others rely on CUDA for their computations.\n",
    "\n",
    "## Common CUDA terminology:\n",
    "- <b>Host:</b> The CPU\n",
    "- <b>Device:</b> The GPU\n",
    "- <b>Host Memory:</b> The system main memory\n",
    "- <b>Device Memory:</b> The GPU onboard memory\n",
    "- <b>kernel:</b> A function that runs on the Device\n",
    "\n",
    "In CUDA threads are organized into a grid of blocks, where each block contains a subset of the threads that can cooperate using a block shared memory and can synchronize within each block.\n",
    "<img src='files/grid1.png' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "\n",
    "Parallel portions of an application are executed on the device (GPU) as kernels, where an array of threads executes each kernel. Each thread has an ID, by which it controls the portion of the data to execute the kernel. All threads run the same code on different portions of the data. Grids and blocks can be organized as 1D, 2D, or 3D arrays. \n",
    "\n",
    "<img src='files/grid2.png' width=\"50%\" height=\"50%\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba\n",
    "CUDA is designed to work with C++, but in this Lab, we will work with Numba; a Python JIT compiler that translates subsets of the code into machine code, and enables writing parallel GPU algorithms in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba installation\n",
    "\n",
    "\n",
    "conda install numba\n",
    "\n",
    "pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel \n",
    "- A Kernel is declared as a function with @cuda.jit decorator.\n",
    "- A Kernel function cannot have a return value and manages outputs as input-output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# kernel decleration\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    # code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To invoke a kernel, you have to specify the number of blocks in the grid, and the number of threads per block. This can be done by specifying the number of threads per block and calculating how many blocks are required in the grid based on the size of the data.\n",
    "\n",
    "Note: In the case that the data size is not divisible by the number of thread per block, we take the ceiling of the number to reserve an extra block for the remaining part of the data. So the threads in the last block will not be fully occupied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# kernel invocation\n",
    "data = np.ones(256)\n",
    "\n",
    "threadsperblock = 32\n",
    "blockspergrid = math.ceil(len(data)/threadsperblock)\n",
    "\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Element-wise operation\n",
    "\n",
    "The following kernel takes a 1D array as input and computes the element-wise cube-root x^(1/3) for each element in the array. This an example of a simple mathematical operation that is, nevertheless, somewhat costly computationally. If you are interested in delving more in-depth, you can replace it with a more straightforward operation, like multiplication, and see what happens.\n",
    "\n",
    "- pos: holds the position in the data on which the thread will work.\n",
    "- always check that the position does not exceed the length of the data, for the cases when the data length is not devisable by the number of threads per block.\n",
    "\n",
    "<img src='files/pos1.png' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "Read the code below and compute the position on which each thread will do its computation in the output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numba as nb\n",
    "import numpy\n",
    "import math\n",
    "\n",
    "# kernel decleration\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    bx = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    \n",
    "    # Compute flattened index inside the array\n",
    "    #TODO: compute the correct pos value based on the tread index and the block index and the block width\n",
    "    pos = 0\n",
    "    \n",
    "    #pos = cuda.grid(1) # this function returns the same value for the position in a 1D grid\n",
    "    \n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] = io_array[pos]**(1/3)\n",
    "        \n",
    "\n",
    "# kernel invocation\n",
    "data = numpy.ones(2048)\n",
    "threadsperblock = 256\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between this kernel and Numpy\n",
    "Try different array sizes and compare computation time between CPU (using numpy) and GPU.\n",
    "Is there a relation between the size of the array and difference in performance? Explain what you notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.ones(10000000)\n",
    "%timeit np.cbrt(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.ones(10000000)\n",
    "threadsperblock = 1024\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "%timeit my_kernel[blockspergrid, threadsperblock](data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Matrix Multiplication\n",
    "\n",
    "In matrix multiplication, every kernel is reponsible for computing one element of the output matrix. It reads one row from the first matrix (A) and one column form the second matrix (B) and computes the dot product of these two vectors and places it in the corresponding cell in the output matrix (C), as shown in the following figure.\n",
    "\n",
    "<img src='files/matmul.png' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "Write a kernel to do the multiplication of two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def mat_mul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    \n",
    "    # find the 2D position of the thread in which it will compute the dot product of the corresponding vectors \n",
    "    row, col = cuda.grid(2)\n",
    "    \n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        #TODO: Compute the dot product \"prod\" of the corresponding vectors of this position \n",
    "        C[row, col] = prod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Create a host function to invoke the kernel\n",
    "\n",
    "It is a good practice to manually copy the matrices to the device (the GPU memory) using \"cuda.to_device\" to reduce the unnecessary data transfer between the device and the host.\n",
    "\n",
    "To test the kernel \"mat_mul\" we prepare the host function \"gpu_dot\" which will take two matrices as parameters and returns the output matrix. The job of this host function is to prepare the data and to invoke the kernel.\n",
    "\n",
    "Read the code below and calculate how many blocks are required to start the kernel. Use the calculated values to invoke the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Test arrays\n",
    "A = np.full((1024, 512), 3, np.float32) # matrix containing all 3's\n",
    "B = np.full((512, 2048), 4, np.float32) # matrix containing all 4's\n",
    "\n",
    "def gpu_dot(A, B):\n",
    "    #Copy the input matrices to the gpu\n",
    "    A_global_mem = cuda.to_device(A)\n",
    "    B_global_mem = 0 # this value should be changed \n",
    "\n",
    "    # Allocate memory on the device for the result (Note the shape of the output matrix)\n",
    "    C_global_mem = cuda.device_array((A.shape[0], B.shape[1]), None) # replace \"None\" with the correct type\n",
    "\n",
    "    # Configure the blocks\n",
    "    # Specify how many threads per block\n",
    "    threadsperblock = (32, 32)\n",
    "    \n",
    "    #TODO: Calculate how many blocks are required\n",
    "    blockspergrid_x = 0 # this value should be changed \n",
    "    blockspergrid_y = 0 # this value should be changed\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    #TODO: Start the kernel based on the calculated grid\n",
    "    \n",
    "    \n",
    "    # Copy the result back to the host\n",
    "    C = C_global_mem.copy_to_host()\n",
    "    return C\n",
    "\n",
    "\n",
    "\n",
    "#Test the host function\n",
    "gpu_dot(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the calculations time compared to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gpu_dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.dot(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Distance Matrix\n",
    "The distance matrix (D) of a data matrix (A) is the matrix that contains the eucleadian distance between each two row vectors as shown in the following figure.\n",
    "<img src='files/distmat.png' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "where \n",
    "$$D[i,j]=D[j,i]=dist(A[i,:], A[j,:])$$\n",
    "\n",
    "\n",
    "Use what you have learned in the previous exercises to write a kernel and a host function to compute the distance matrix of a given data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit(\"void(float64[:, :], float64[:, :])\")\n",
    "def distance_matrix(mat, out):\n",
    "    #TODO: write a kernel to compute the distance matrix of the input \"mat\" and place the result in \"out\"\n",
    "\n",
    "def gpu_dist_matrix(mat):\n",
    "    #TODO: write a host function to calculate the grid size and use the calculated values to invoke the \"distance_Matrix\" kernel\n",
    "    return out\n",
    "\n",
    "\n",
    "A = np.random.randn(1024,1024)\n",
    "D = gpu_dist_matrix(A)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Convolution\n",
    "\n",
    "Write a convolution kernel and host function that convolves a grayscale image with a filter and compare the performance with the convolution method in scipy \"scipy.ndimage.filters.convolve\".\n",
    "\n",
    "<img src='files/conv.png' width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_kernel(image, cfilter, out_image):\n",
    "    # TODO: write a kernel to convolve the image with the filter and produce a the filtered image out_image \n",
    "    \n",
    "def gpu_conv(image, cfilter):\n",
    "    # write a host function to invoke the kernel conv_kernel\n",
    "    return out_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the convolution kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "#Loading the image and converting it to numpy array\n",
    "im = Image.open(\"files/QCaSg.png\")\n",
    "im_arr = np.array(im.getdata()).reshape(im.size[0], im.size[1])\n",
    "plt.imshow(im_arr, cmap='gray')\n",
    "print(\"Image Shape:\",im_arr.shape)\n",
    "\n",
    "# Creating the filter to use\n",
    "filt = np.array([[-1,-1,-1], [-1,8,-1], [-1,-1,-1]])\n",
    "print(\"Filter:\\n\",filt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import convolve\n",
    "\n",
    "# Using scipy convolution\n",
    "filtered_image = convolve(im_arr, filt)\n",
    "\n",
    "plt.imshow(filtered_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Development of a Simple Neural Network\n",
    "\n",
    "The purpose of using GPUs for training in deep learning is its ability to parallelize matrix computation on which deep learning models heavily rely. \n",
    "\n",
    "For the purpose of this Lab, we will use the file NN.py, which contains an MLP class used to create a multi-layer neural network. The class MLP accepts as input the network structure (as a dictionary object), and a matrix multiplication function to use in the computations. By default, the matrix multiplication function it uses is \"numpy.dot\". We can try the function implemented in Example 2 and compare it with NumPy.\n",
    "\n",
    "Read the file NN.py, and implement the missing parts (places marked with TODO keywords). The file is a full implementation of the neural network that consists of feedforward and backpropagation for training the neural networks. The backward calculations involve the computation of the gradients of the loss function with respect to the weights of the network dL/dw. This part is implemented, but you are encouraged to read it carefully to understand how it is calculated.\n",
    "\n",
    "Most of the parts that you are required to calculate are in the feedforward step. Read the comments carefully to be able to know the kind of output that is expected from each method.\n",
    "\n",
    "After completing the code in NN.py file, use the code below to verify that it is working. Use the matrix multiplication function that you have developed earlier and compare it with the use of Numpy multiplication. Explain what you notice.\n",
    "\n",
    "Write a short report in the section below, explaining your observations. Send the report to Abdallah Alabdallah <abdallah.alabdallah@hh.se> by Friday, 1 May 2020 at 23:59.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NN import MLP\n",
    "import numpy as np\n",
    "\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "# matmul holds the function to be used for matrix multiplication, it can be np.dot or gpu_dot\n",
    "model = MLP(NN_ARCHITECTURE, matmul=gpu_dot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "\n",
    "# Create the Data\n",
    "X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "# Training\n",
    "model.train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), 10000, 0.01, verbose=True)\n",
    "# Prediction\n",
    "Y_test_hat = model.predict(np.transpose(X_test))\n",
    "\n",
    "# Accuracy achieved on the test set\n",
    "acc_test = model.get_accuracy_value(Y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gpu_dist_matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gpu_dot(A,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.dot(A,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report:\n",
    "Send the report to Abdallah Alabdallah <abdallah.alabdallah@hh.se> by Friday, 1 May 2020 at 23:59.\n",
    "\n",
    "### Name:\n",
    "\n",
    "### Exercise 1:\n",
    "\n",
    "### Exercise 2:\n",
    "\n",
    "### Exercise 3:\n",
    "\n",
    "### Exercise 4:\n",
    "\n",
    "### Exercise 5:\n",
    "\n",
    "### Homework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
